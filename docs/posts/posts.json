[
  {
    "path": "posts/2021-12-01-opendata-per-canton/",
    "title": "How open are our cantons?",
    "description": "Fedaralistism at its best: How the open data policy is handled across cantons",
    "author": [
      {
        "name": "Nils Ratnaweera",
        "url": {}
      }
    ],
    "date": "2021-12-01",
    "categories": [],
    "contents": "\n\n\n\nJust a couple of years ago, we never used geodata from cantons when doing a Swiss wide project. The data was just too inhomogeneous, too hard to assemble and to expensive to acquire. Now, with geodienste.ch up and running, this task has become much much simpler. Or so I thought.\nTo be fair, the platform is amazing (although lacking an API) and most cantons are very accommodating, providing the data freely. Some cantons however, are still extremely restrictive with their publicly funded data, asking for close to 50’000 CHF (!!) for commercial use of their data! I’m lucky enough to be working in a research project at a university, but I can’t help but mourn the opportunities this data could be used for if it was provided freely.\nAnyway, this experience lead me to the question: Which cantons are most open in their data sharing policy? Which cantons are more restrictive? To answer this question, I scraped the title-page of each of the 23 datasets on geodienste.ch and visualized the results.\ngeodienste.ch provides three different methods to obtain the data. Either 1) the data is freely available without registration, 2) the data is freely available after registering on the website or 3) the canton needs to approve your request. This last method can mean that you will be grated approval within a few hours, or that you need to wait a couple of days to receive an email asking you to pay horrendous amounts (if you want to use it commercially).\nThe results show that most datasets available on the website are offered freely and without the need of registration. Some few datasets require registration, and only 6 cantons feel the need to manually approve and potentially charge certain datasets. Foremost, the cantons Jura, Ticino and Valais heavily guard their data and require approval on a large number of their datasets.\nMost cantons offer between 10 and 15 datasets on geodienste.ch. The canton Schwyz has the highest number of datasets online (20), while Zug, Bern and Schaffhausen share second place with 18 datasets each. All provide their data to unregistered users freely. Good for you, that is the way to go!\n\n\nshow code for webscraping\n\nlibrary(httr)\nlibrary(xml2)\nlibrary(rvest)\nlibrary(tidyverse)\n\nservices <- read_html(\"https://geodienste.ch/versions_overview\") %>%\n  html_elements(\"a\") %>%\n  html_attr(\"href\")\n\nservices <- services[str_detect(services, \"/services/\")]\n\n\nkantone <- read_html(\"https://geodienste.ch/services/av\") %>%\n  html_elements(\".wappen-kt\") %>%\n  html_text()\n\nkantone <- str_trim(kantone)\nwappen <- read_html(\"https://geodienste.ch/services/av\") %>%\n  html_elements(\".wappen-kt\") %>%\n  html_elements(\"img\") %>%\n  html_attr(\"src\")\n\nkantone2 <- paste0(kantone,str_extract(wappen, \"\\\\.\\\\w{3}$\"))\n\n\nmap2(kantone2, wappen, function(x,y){\n  download.file(paste0(\"https://geodienste.ch/\",y),file.path(\"wappen\",x))\n})\n\nfi <- list.files(\"wappen/\",\".png\", full.names = TRUE)\n\nfile.rename(fi, str_remove(fi, \" \"))\n\n\nmyres <- map(services, function(url_i){\n  res <- read_html(paste0(\"https://geodienste.ch/\",url_i)) %>%\n    # html_elements(\".canton-table\") %>%\n    html_table() %>%\n    magrittr::extract2(1)\n  \n  res <- res %>% \n    janitor::clean_names()\n  \n  res %>%\n    filter(!is.na(info))\n})\n\nres2 <- map2_dfr(myres, services, function(mydf, serv){\n  mydf <- mydf[,1:3]\n  mydf$services <- serv\n  mydf\n})\n\nwrite_csv(res2, \"geodienste-raw.csv\")\n\n\n\n\n\nshow code for data preparation\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(cowplot)\n\n\nres2 <- read_csv(\"geodienste-raw.csv\")\nfacs <- c(\"Frei erhältlich\",\"Registrierung erforderlich\",\"Freigabe erforderlich\",\"Im Aufbau\", \"keine Daten\")\n\nres3 <- res2 %>%\n  separate(verfugbarkeit, c(\"verfugbarkeit\",\"verfugbarkeit2\"), sep = \"\\\\n\\\\s+\") %>% \n  select(-verfugbarkeit2) %>%# verfugbarkeit2 seems to be erroneous \n  mutate(\n    erhaeltlich_ab = str_extract(verfugbarkeit, \"\\\\d{2}\\\\.\\\\d{2}\\\\.\\\\d{4}\"),\n    verfugbarkeit = str_remove(verfugbarkeit, \"\\\\s\\\\(.+\\\\)\")\n  ) %>%\n  rename(kanton = x)\n\n\nres3_wide <-res3 %>%\n  group_by(kanton, verfugbarkeit) %>%  \n  count() %>%\n  mutate(verfugbarkeit_code = paste0(\"verf\",as.integer(factor(verfugbarkeit, facs)))) %>%\n  ungroup() %>%\n  select(-verfugbarkeit) %>%\n  pivot_wider(names_from = verfugbarkeit_code, values_from = n,values_fill = 0) %>%\n  select(kanton, order(colnames(.))) %>%\n  arrange(across(starts_with(\"verf\")))\n\n\nres4 <- res3 %>%\n  mutate(\n    kanton = factor(kanton, levels = res3_wide$kanton, ordered = TRUE),\n    verfugbarkeit = factor(verfugbarkeit, levels = facs, ordered = TRUE),\n    x = 1\n  ) \n\n\nwappen_df <- tibble(file = list.files(\"wappen\",full.names = TRUE)) %>%\n  mutate(\n    kanton = str_extract(file, \"[A-Z][A-Z]\"),\n    file_read = list(grid::rasterGrob(png::readPNG(file)))\n) %>%\n  left_join(res3_wide, ., by = \"kanton\") %>%\n  mutate(y = row_number())\n\n\n\n\n\nshow code for creating the plot\n\ncols <- rev(RColorBrewer::brewer.pal(5, \"RdYlGn\"))\nres4 %>%\n  ggplot(aes(x, kanton, fill = verfugbarkeit)) + \n  geom_col(position = position_stack(reverse = TRUE), color = \"white\") +\n  pmap(select(wappen_df, file, y), function(file, y){draw_image(file, x = -0, y = y, width = 0.8, height = 0.6,hjust = 1,vjust = 0.5)}) +\n  scale_fill_manual(\"Status\",values = cols) +\n  scale_x_continuous(\"Anzahl Datensätze\",sec.axis = sec_axis(~./23,\"Anteil der Datensätze\", labels = scales::percent_format())) +\n  # guides(fill = guide_legend(title.position = \"top\",title.hjust = 0.5)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",axis.title.y = element_blank(), panel.grid = element_blank(),legend.text = element_text(size = 7),legend.title = element_blank()) +\n  coord_equal()\n\n\n\n\nFigure 1: All labels are in german for practical reasons. The labels (from left to right): 1) freely availalbe, 2) registration necessary, 3) approval necessary, 4) in the making, 5) no data\n\n\n\n\n\n\n",
    "preview": "posts/2021-12-01-opendata-per-canton/opendata-per-canton_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-12-02T21:46:41+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1920
  },
  {
    "path": "posts/2021-11-05-warping-switzerland-back-into-shape/",
    "title": "Warping Switzerland back into shape",
    "description": "What changed when switching from Switzerland's old coordinate system to the new one?",
    "author": [
      {
        "name": "Nils Ratnaweera",
        "url": {}
      }
    ],
    "date": "2021-11-05",
    "categories": [],
    "contents": "\n\n.code-highlight{\n  background-color: #3498DB;\n}\n\n\n\nWhen I started working geodata a couple of years ago, Switzerland was beginning to transition from it’s old coordinate system CH1903 LV03 to CH1903+ LV95 (EPSG 2056).\nThe new coordinate system was awkward from the start. First off, I suddenly was un unable to convert data from WGS84 using a custom R script that swisstopo had previously provided for CH1903 LV03. The answer to my question on a swisstopo google group (in 2017) was to use swisstopo’s REST api.\nThis problem was seemingly solved with the emergence of sf and it’s built-in methods to reproject coordinates easily. Until one day I realized that the coordinate transformations to and from EPSG 2056 were fairly imprecise. The transformations did not account for the spatially varying offset for which the new coordinate system was implemented in the first place.\nI filed an issue on sf’ github repo beginning of 2020, but it wasn’t until recently (with PROJ 7.0.0) that I was able to precisely transform my data to and from EPSG 2056.\nThe issue is highlighted with the function list_coordOps() from rgdal. Apparently, the transformation was lacking a grid with the name ch_swisstopo_CHENyx06a.tif.\n\n\nShow code\n\nrgdal::list_coordOps(\"EPSG:21781\", \"EPSG:2056\")\n\n\n\n\n\nCandidate coordinate operations found:  2 \nStrict containment:  FALSE \nVisualization order:  TRUE \nSource: EPSG:21781 \nTarget: EPSG:2056 \nBest instantiable operation has only ballpark accuracy \nDescription: Inverse of Swiss Oblique Mercator 1903M + Ballpark geographic \noffset from CH1903 to CH1903+ + Swiss Oblique Mercator\n             1995\nDefinition:  +proj=pipeline +step +inv +proj=somerc +lat_0=46.9524055555556 \n             +lon_0=7.43958333333333 +k_0=1 +x_0=600000+y_0=200000 +ellps=bessel \n             +step +proj=somerc +lat_0=46.9524055555556 +lon_0=7.43958333333333 \n             +k_0=1 +x_0=2600000 +y_0=1200000 +ellps=bessel\nOperation 1 is lacking 1 grid with accuracy 0.2 m\nMissing grid: ch_swisstopo_CHENyx06a.tif \nURL: https://cdn.proj.org/ch_swisstopo_CHENyx06a.tif \n\n\nWith my current version of sf and rgdal, I need to download this grid manually and copy it to the PROJ directory. Running list_coordOps now shows a different output.\n\n\nShow code\nwget https://cdn.proj.org/ch_swisstopo_CHENyx06a.tif\nsudo mv ch_swisstopo_CHENyx06a.tif /usr/share/proj/\n\n\n\nShow code\n\nrgdal::list_coordOps(\"EPSG:21781\", \"EPSG:2056\")\n\n\nCandidate coordinate operations found:  2 \nStrict containment:  FALSE \nVisualization order:  TRUE \nSource: EPSG:21781 \nTarget: EPSG:2056 \nBest instantiable operation has accuracy: 0.2 m\nDescription: Inverse of Swiss Oblique Mercator 1903M + CH1903 to\n             CH1903+ (1) + Swiss Oblique Mercator 1995\nDefinition:  +proj=pipeline +step +inv +proj=somerc\n             +lat_0=46.9524055555556\n             +lon_0=7.43958333333333 +k_0=1 +x_0=600000\n             +y_0=200000 +ellps=bessel +step\n             +proj=hgridshift\n             +grids=ch_swisstopo_CHENyx06a.tif +step\n             +proj=somerc +lat_0=46.9524055555556\n             +lon_0=7.43958333333333 +k_0=1\n             +x_0=2600000 +y_0=1200000 +ellps=bessel\n\nThe downloaded file is a so called transformation grid: A raster dataset in WGS84 containing information on the lat and lon offset for a given cell.\n\n\nShow code\n\nlibrary(tmap)\nlibrary(sf)\nlibrary(terra)\nlibrary(cowplot)\n\nch_swisstopo_CHENyx06a <- terra::rast(\"ch_swisstopo_CHENyx06a.tif\")\n# Swissboundaries3D data from swisstopo\nlandesgebiet <- read_sf(\"data-git-lfs/swissboundaries/swissBOUNDARIES3D_1_3_TLM_LANDESGEBIET.shp\")\n\nlandesgebiet <- st_transform(landesgebiet, 2056)\n\n\n\ntm_shape(ch_swisstopo_CHENyx06a) + \n  tm_raster(style = \"cont\",palette = \"RdYlBu\") + \n  tm_shape(landesgebiet) + tm_borders()\n\n\n\n\nI now can use this grid to precisely transform coordinates to and from ESPG 2056. I will demonstrate this by visualizing the directional offset, similar to the transformation grid I downloaded.\n\n\nShow code\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\nheading_north <- function(h){\n    case_when(\n        h>=0 & h<=90~90-h,\n        h>90~450-h,\n        TRUE~abs(h) + 90\n      )\n  }\n\nmake_grid <- function(cellsize, land){\n  \n  land <- st_transform(land, 2056)\n  \n  gri <- st_make_grid(land, cellsize, what = \"centers\")\n\n  gri2 <- st_transform(gri, 21781)\n  \n  mat <- cbind(\n    st_coordinates(gri),\n    st_coordinates(gri2)\n  )\n  \n  colnames(mat) <- c(\"E\", \"N\", \"X\", \"Y\")\n  \n  \n\n  \n  mat %>%\n    tibble::as_tibble() %>%\n    mutate(\n      E_norm = E - (X+2e6),\n      N_norm = N - (Y+1e6),\n      dist = (E_norm^2+N_norm^2)^0.5,\n      heading = atan2(N_norm, E_norm),\n      h = heading*180/pi,\n      heading_deg = heading_north(h)\n    )\n  \n}\n\n\n\n\n\nShow code\n\n# Add some cities as labels\nmarkers <- tribble(\n  ~E, ~N, ~name, ~nudge_x, ~nudge_y,\n  2598633.750, 1200386.750, \"Bern\",15000,15000,\n  2682217.000, 1247945.250 , \"Zurich\",15000,-15000,\n  2598410, 1185268, \"apparent epicenter\",15000,-15000,\n)\n\n# Make my custom grid, similar to the transformation grid\ndf <- make_grid(1000, landesgebiet)\n\n# https://coolors.co/ef476f-ffd166-06d6a0-118ab2-073b4c\nmycols <- paste0(\"#\",c(\"EF476F\",\"FFD166\",\"06D6A0\", \"118AB2\",\"EF476F\"))\n\n\n\n\n\nShow code\n\nmygrid <- make_grid(4000, landesgebiet) %>%\n  st_as_sf(coords = c(\"E\",\"N\"), remove = FALSE) %>%\n  st_set_crs(2056)\n\nmygrid <- mygrid[landesgebiet, , op = st_intersects]\n\nbgcol <- \"#073B4C\"\n\nmain_plot <- ggplot(mygrid, aes(E, N, color = heading_deg)) +\n  # geom_sf(data = bb_ch, inherit.aes = FALSE, fill = bgcol, color = \"NA\")  +\n  geom_point(size = .4) +\n  geom_spoke(aes(angle = heading, radius = scales::rescale(dist, 5e3, 10e3))) +\n  scale_color_gradientn(colors = mycols) +\n  labs(title = \"Warping Switzerland back into shape\",\n       subtitle = \"How off were the old coordinates?\",\n       caption = \"The spokes show the offset between EPSG 21781 and EPSG 2056.\\nData from swisstopo. Visualized by Nils Ratnaweera\") +\n  coord_equal() +\n  theme_void() +\n  theme(legend.position = \"none\", \n        plot.title = element_text(hjust = .5),\n        plot.subtitle = element_text(hjust = .5),\n        text = element_text(color = \"white\"),\n        plot.margin = margin(10,10,10,10),\n        panel.background = element_rect(colour = NA, fill = bgcol),\n        plot.background = element_rect(fill = bgcol,color = NA))\n\n\ndeg2rad <- function(deg){deg*pi/180}\n\nlegend_df <- tibble(\n  angle_deg1 = seq(0,359,10),\n  angle_deg2 = heading_north(angle_deg1),\n  angle_rad = deg2rad(angle_deg2),\n  x = cos(angle_rad),\n  y = sin(angle_rad)\n  )\n\nfac = 2\nwindrad <- tibble(\n  angle_deg = c(0,90,180,270),\n  text = c(\"N\",\"E\",\"S\",\"W\"),\n  angle_rad = deg2rad(heading_north(angle_deg)),\n  x = cos(angle_rad)*fac,\n  y = sin(angle_rad)*fac\n)\n\nlegend_plot <- ggplot(legend_df, aes(x, y, color = angle_deg1)) +\n  geom_point(size = .8) +\n  geom_text(data = windrad, aes(label = text), color = \"white\", size = 2) +\n  geom_spoke(aes(angle = angle_rad,radius = .6)) +\n  scale_color_gradientn(colors = mycols) +\n  coord_equal() +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nShow code\n\nggdraw(main_plot) +\n  draw_plot(legend_plot, .90, .95, .2, .2, scale = 1, \n            hjust = 1,vjust = 1, halign = 1, valign = 3)\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-11-05-warping-switzerland-back-into-shape/distill-preview.png",
    "last_modified": "2021-11-05T00:41:29+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-27-r-usergroup-zurich-swisstopo-challenge/",
    "title": "R Usergroup Zurich",
    "description": "Swisstopo challenge",
    "author": [
      {
        "name": "Nils Ratnaweera",
        "url": {}
      }
    ],
    "date": "2021-10-27",
    "categories": [],
    "contents": "\nAfter a long time the Zurich R User Group meets up again physically. The organisers posted the following message on their board:\n\n\n\nSince I regularly do things with these datasets in R, I decided to present some of my use cases. Here are the slides for this talk:I\nFull screen: ratnanil.github.io/r-usergroup-swisstopo\nRepo (not much R-code here): github.com/ratnanil/r-usergroup-swisstopo \n\n\n\n",
    "preview": "https://raw.githubusercontent.com/ratnanil/r-usergroup-swisstopo/main/images/overlay.png",
    "last_modified": "2021-10-27T11:25:24+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-22-benchmarking-binary-predicates/",
    "title": "Benchmarking binary predicates",
    "description": "Comparing the performance of different methods to do a \"point in polygon operation\" with sf.",
    "author": [
      {
        "name": "Nils Ratnaweera",
        "url": {}
      }
    ],
    "date": "2021-10-22",
    "categories": [
      "R",
      "geodata"
    ],
    "contents": "\nI often work with geodata in R and come across situations where I need to subset points based on whether they lie within a polygon or not. There are several functions to solve this problem.1 From the package sf, the functions st_within, st_contains, st_intersects and st_covered_by can all answer this question.2 I noticed that with big datasets, some of these functions are unbearably slow. To find out which one is faster in which scenario, I decided to benchmark these four functions.\nTo make things more interesting, I won’t use my usual Swiss data for this test, but data from my second home, Sri Lanka. More specifically: I will use the Geonames data (> 50k points) and the administrative boundaries of Sri Lanka (26 polygons).\n\n\nshow code for: loading libraries\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(sf)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(microbenchmark)\nlibrary(ggridges)\nlibrary(forcats)\n\n\n\n\n\nshow code for: preparing boundary data\n\n# Downloaded from: https://data.humdata.org/dataset/sri-lanka-administrative-levels-0-4-boundaries\n# Administrative Level 0: country (1 features)\n# Administrative Level 1: province (9 features)\n# Administrative Level 2: district (26 features)\n# Administrative Level 3: divisional secretatiat (333 features)\n# Administrative Level 4: grama niladhari (14'044 features)\n\ntmp <- tempdir()\n\nboundary_dir <- file.path(tmp, \"boundary\")\nunzip(\"data-git-lfs/lka_adm_slsd_20200305_shp.zip\", exdir = boundary_dir)\n\nsl_boundary_l2 <- read_sf(\n  file.path(boundary_dir, \"lka_admbnda_adm2_slsd_20200305.shp\")\n  )\n# https://epsg.io/5234\n# https://epsg.io/5235\n\n\n\n\n\nshow code for: preparing geonames dataset\n\n# geonameid         : integer id of record in geonames database\n# name              : name of geographical point (utf8) varchar(200)\n# asciiname         : name of geographical point in plain ascii characters, varchar(200)\n# alternatenames    : alternatenames, comma separated, ascii names automatically transliterated, convenience attribute from alternatename table, varchar(10000)\n# latitude          : latitude in decimal degrees (wgs84)\n# longitude         : longitude in decimal degrees (wgs84)\n# feature class     : see http://www.geonames.org/export/codes.html, char(1)\n# feature code      : see http://www.geonames.org/export/codes.html, varchar(10)\n# country code      : ISO-3166 2-letter country code, 2 characters\n# cc2               : alternate country codes, comma separated, ISO-3166 2-letter country code, 200 characters\n# admin1 code       : fipscode (subject to change to iso code), see exceptions below, see file admin1Codes.txt for display names of this code; varchar(20)\n# admin2 code       : code for the second administrative division, a county in the US, see file admin2Codes.txt; varchar(80) \n# admin3 code       : code for third level administrative division, varchar(20)\n# admin4 code       : code for fourth level administrative division, varchar(20)\n# population        : bigint (8 byte int) \n# elevation         : in meters, integer\n# dem               : digital elevation model, srtm3 or gtopo30, average elevation of 3''x3'' (ca 90mx90m) or 30''x30'' (ca 900mx900m) area in meters, integer. srtm processed by cgiar/ciat.\n# timezone          : the iana timezone id (see file timeZone.txt) varchar(40)\n# modification date : date of last modification in yyyy-MM-dd format\n\n\ncolnames <- c(\"geonameid\", \"name\",  \"asciiname\",  \"alternatenames\", \"latitude\",  \n              \"longitude\",  \"feature_class\",  \"feature_code\", \"country_code\",  \n              \"cc2\",  \"admin1_code\",  \"admin2_code\",  \"admin3_code\",  \n              \"admin4_code\",  \"population\", \"elevation\", \"dem\", \"timezone\",  \n              \"modification_date\")\n\n\ngeonames_dir <- file.path(tmp, \"geonames\")\n\nunzip(\"data-git-lfs/LK.zip\", exdir = geonames_dir)\n\ngeonames <- read_tsv(file.path(geonames_dir, \"LK.txt\"),col_names = colnames) %>%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\n\nOnce all the data is imported, I can demonstrate visually the task. I want to subset all points within the province of Kandy (incidentally where I spent 5 superb years of my childhood). Using st_within() for this operation, the output looks like this:\n\n\n\nshow code for: subsetting and creating a map\n\nkandy <- filter(sl_boundary_l2, ADM2_EN == \"Kandy\")\n\npoints_filter <- list(\n  within = geonames[st_within(geonames,kandy,sparse = FALSE)[,1],]\n)\n\n\np1 <- ggplot(sl_boundary_l2) + \n  geom_sf(color = \"#ffffff\", fill = \"#ababab\") +\n  geom_sf(data = rbind(transmute(geonames, val = \"all points\"), \n                       transmute(points_filter[[\"within\"]], \n                                 val = \"points within the\\nprovince of Kandy\")), \n          alpha = 0.05, size = 0.05, color = \"#8d2663\") +\n  geom_sf(data = ~filter(., ADM2_EN == \"Kandy\"), fill = NA, color = \"#000000\") +\n  facet_wrap(~val) +\n  coord_sf(xlim = c(78, 83)) + \n  theme(strip.background = element_blank(),\n        strip.text = element_text(color = \"white\"),\n        panel.background = element_blank(),\n        plot.background = element_rect(fill = \"#2d2d2d\"),\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        )\n\np1\n\n\n\n\nNext, I will do the same operation with the other functions and also check the output number of rows to see if they are similar (they might be slightly off if we have points exactly on the polygon boundary) or even identical.\n\n\npoints_filter[[\"contains\"]] <- geonames[st_contains(kandy,\n                                                    geonames,\n                                                    sparse = FALSE)[1,],]\npoints_filter[[\"intersects\"]] <- geonames[st_intersects(geonames,\n                                                        kandy,\n                                                        sparse = FALSE)[,1],]\npoints_filter[[\"covered_by\"]] <- geonames[st_covered_by(geonames,\n                                                        kandy,\n                                                        sparse = FALSE)[,1],]\n\ntibble(\n  function_name = names(points_filter),\n  nrow = sapply(points_filter, nrow),\n  identical_to_st_within = sapply(points_filter, function(x){\n    identical(points_filter[[\"within\"]], x)\n    })\n) %>%\n  knitr::kable(col.names = stringr::str_replace_all(colnames(.),\"_\", \" \"))\n\n\nfunction name\nnrow\nidentical to st within\nwithin\n3251\nTRUE\ncontains\n3251\nTRUE\nintersects\n3251\nTRUE\ncovered_by\n3251\nTRUE\n\nTo find out which function is the fastest, I use the package microbenchmark. Since it doesn’t always take the same amount of time to process the same function, each function is executed multiple times (times = 50) and we will look at the distribution of the execution times.\n\n\nshow code for: Benchmarking the functions\n\nmbm  <- microbenchmark(\n  intersects = st_intersects(kandy,geonames),\n  within = st_within(geonames,kandy),\n  contains = st_contains(kandy,geonames),\n  covered_by = st_covered_by(geonames,kandy),\n  times = 50\n)\n\n\n\n\n\n\n\n\n\n\n\nshow code for: visualizing the result\n\nmbm2df <- function(mbm_obj){\n  df <- as.data.frame(mbm_obj)\n  df$time <- dnanoseconds(df$time)\n  df\n}\n\nmbm_df <- mbm2df(mbm)\n\n\np2 <- mbm_df %>%\n  mutate(\n    expr = fct_reorder(expr,time,median,.desc = TRUE)\n  ) %>%\n  ggplot(aes(time,expr,fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01) +\n  scale_fill_viridis_c(option = \"C\")  +\n  scale_x_time(name = \"Duration (in seconds)\",\n               labels = scales::time_format(format = \"%OS3\")) +\n  labs(y = \"Function\") +\n  theme_minimal() +\n  theme(legend.position=\"none\")\n\np2\n\n\n\n\nThis benchmark shows that st_contains and st_intersects executes faster than st_covered_by and st_within. The next question is: How do the functions scale and perform under different scenarios? I’ll test this by generating additional points to subset, and also by using more provinces than just Kandy. And since I’m more interested in relative rather than absolute execution times, I will calculate the median duration per function and scenario and rescale the values by deviding them with the duration of st_intersects.\n\n\nshow code for: Benchmarking scalability\n\nn_points_vec <- c(100e3,200e3,500e3)\nn_poly_vec <- c(1,9,17,26)\n\nmbm2 <- map_dfr(n_points_vec,function(n_points){\n  \n  points <- st_sample(sl_boundary_l2,n_points,what = \"centers\")\n\n  mbm_points <- map_dfr(n_poly_vec, function(n_poly){\n    \n    polygons <- sample_n(sl_boundary_l2, n_poly)\n    \n    mbm_poly <- microbenchmark(\n      intersects = st_intersects(polygons,points),\n      within = st_within(points,polygons),\n      contains = st_contains(polygons,points),\n      covered_by = st_covered_by(points,polygons),\n      times = 10\n      )\n\n    as_tibble(mbm_poly) %>%\n      mutate(n_poly = n_poly)\n  }) %>%\n    mutate(n_points = n_points)\n})\n\n\n\n\n\n\n\n\n\n\n\nshow code for: Visualizing results\n\nmbm2_df <- mbm2df(mbm2)\n\n\n\nmylabels <- function(x){sprintf(\"%+3.f%%\", x*100)}\n\nmbm2_df %>%\n  group_by(expr, n_poly, n_points) %>%\n  summarise(median = median(time)) %>% \n  ungroup() %>%\n  group_by(n_poly,n_points) %>%\n  mutate(\n    perc = median/median[expr == \"contains\"]-1,\n    expr = fct_relevel(expr, \"within\", \"covered_by\",\"intersects\", \"contains\")\n    ) %>%\n  ggplot(aes(perc,as.factor(expr), color = expr, fill = expr)) +\n  geom_point() +\n  geom_linerange(aes(xmin = 0, xmax = perc)) +\n  # expand_limits(x = 0) +\n  scale_x_continuous(\"Relative execution time (compared to 'st_contains')\", \n                     breaks = seq(-.4,.4,.2), labels = mylabels,\n                     limits = c(-.5,.5), \n                     sec.axis = sec_axis(~.x, \n                                         breaks = c(-.4,.4), \n                                         labels = c(\"< faster\",\"slower > \"))) +\n  labs(y = \"\") +\n  facet_grid(n_poly~n_points, \n             labeller = labeller(n_points = ~paste0(as.integer(.x)/1e3, \"K points\"),\n                                 n_poly = ~paste0(.x, \" polygons\")))+\n  theme_light() +\n  theme(legend.position = \"none\", \n        axis.ticks.x.top = element_blank(), \n        text = element_text(size = 9))\n\n\n\n\nThis test shows something interesting: While st_contains and st_intersects are fast with a single polygon, they don’t scale well with lager number of polygons. This effect is especially prominent when the number of points is large (~500K).\nMy take home message from this whole exercise: If you want to subset points based on whether they lie in specific polygons or not, use st_intersects or st_contains when the number of polygons is small. Use st_covered_by or st_within when the number of polygons is large. If it’s important what happens to points lying on the edge, remember that only st_intersects and st_covered_by will include them.\n\nespecially since I mostly don’t care what happens to point which lie exactly on the polygon edge↩︎\nst_within and st_contains will disregard points on a line, st_intersects and st_covered_by will include them↩︎\n",
    "preview": "https://nils.ratnaweera.net/posts/2021-10-22-benchmarking-binary-predicates/preview.jpg",
    "last_modified": "2021-10-22T07:15:37+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-13-minimalistic-topography/",
    "title": "Minimalistic topography",
    "description": "A beautiful way to visualize topography, inspired by Carla Martínez Sastre",
    "author": [
      {
        "name": "Nils Ratnaweera",
        "url": {}
      }
    ],
    "date": "2021-08-13",
    "categories": [
      "R",
      "geodata"
    ],
    "contents": "\n“So beautiful that it hurts”1 Bauhasaurus wrote in his tweet, posting an image by Carla Martínez Sastre. The artist had used a beautiful, clever and minimalistic way to visualize the topography of South America.\n\n \n\nThe way I understand it, Carla drew “horizontal” (latitudanal) elevation profiles at equal intervals over the continent and filled these elevation profiles to visualize not only the continent’s topography, but also implicitly showing it’s borders.\nI found this a very nice approach and tried recreating this idea with R for my home country, Switzerland. I’m quite happy with the result, however there is still a lot of room for improvement. I’ve packed the approach into generic functions, see below for the complete source code. Check below to see the source code.\n\n\n\nCreate some generic functions\n\n\n#' Create ridgelines from a digital elevation model (dhm)\n#'\n#' dhm: path to a dhm that can be imported using terra::rast\n#' n_lines: how many lines / polygons do you want to draw? Default is 50\n#' vspace: vertical space between lines, in units provided by the dhm. This overrides n_lines\n#' fac: How much of the space between the lines should be occupied by the hightest elevation?\n#' point_density: Density of the point samples used to extract elevation. Defaults to the inverse of the raster resolution\n#' geom_type: What should the output geometry type be? Can be LINESTRING or POLYGON\ncreate_ridges <- function(dhm, n_lines = 50, vspace = NULL, fac = 2, point_density = NULL, geom_type = \"LINESTRING\"){\n  \n  library(sf)\n  library(terra)\n  library(purrr)\n  \n  # extract the extent of the dhm as a vector\n  ex <- ext(dhm) %>%\n    as.vector()\n  \n  # If vspace is NULL (default), then vspace is calculated using n_lines\n  if(is.null(vspace)){\n    vspace <- (ex[\"ymax\"] - ex[\"ymin\"])/n_lines\n  }\n  \n  \n  point_density <- if(is.null(point_density)){1/terra::res(dhm)[2]}\n  \n  # Defines at what y-coordinates elevation should be extracted\n  heights <- seq(ex[\"ymin\"], ex[\"ymax\"], vspace)\n  \n  # calculates the x/y coordinates to extract points from the dhm\n  mypoints_mat <- map(heights, function(height){\n    matrix(c(ex[\"xmin\"], height, ex[\"xmax\"], height), ncol = 2, byrow = TRUE) %>%\n      st_linestring()\n  }) %>%\n    st_as_sfc() %>%\n    st_line_sample(density = point_density,type = \"regular\") %>%\n    st_as_sf() %>%\n    st_cast(\"POINT\") %>%\n    st_coordinates()\n  \n  \n  # extracts the elevation from the dhm\n  extracted <- terra::extract(dhm, mypoints_mat) %>% \n    cbind(mypoints_mat) %>% \n    as_tibble()\n  \n  # calculates the factor with which to multiply elevation, based on \"fac\" and the maximum elevation value\n  fac <- vspace*fac/max(extracted[,1], na.rm = TRUE)\n  \n  # calculates the coordinats of the ridge lines\n  coords <-extracted %>%\n    filter(!is.na(extracted[,1])) %>%\n    split(.$Y) %>%\n    imap(function(df, hig){\n      hig <- as.numeric(hig)\n      Y_new <- hig+pull(df[,1])*fac\n      matrix(c(df$X, Y_new), ncol = 2)\n    })\n\n  # creates LINESTRING or POLYGON, based on the \"geom_type\"\n  geoms <- if(geom_type == \"LINESTRING\"){\n    map(coords, ~st_linestring(.x))\n  } else if(geom_type == \"POLYGON\"){\n    imap(coords, function(x, hig){\n      hig <- as.numeric(hig)\n      \n      first <- head(x, 1)\n      first[,2] <- hig\n      last <- tail(x, 1)\n      last[,2] <- hig\n      \n      st_polygon(list(rbind(first, x, last, first)))\n    })\n  } else{\n    stop(paste0(\"This geom_type is not implemented:\",geom_type,\". geom_type must be 'LINESTRING' or 'POLYGON'\"))\n  }\n  \n  # adds the CRS to the output sfc\n  dhm_crs <- crs(dhm)\n  \n  if(dhm_crs == \"\") warning(\"dhm does not seem to have a CRS, therefore the output does not have a CRS assigned either.\")\n  \n  geoms %>%\n    st_sfc() %>%\n    st_set_crs(dhm_crs)\n  \n}\n\n# A helper function to creteate a polygon from the extent of a (dhm) raster\nst_bbox_rast <- function(rast_obj){\n  \n  library(terra)\n  library(sf)\n  \n  ex <- ext(rast_obj) %>%\n    as.vector()\n  \n  matrix(c(ex[1],ex[3],ex[1], ex[4],ex[2], ex[4],ex[2],ex[3],ex[1],ex[3]),ncol = 2, byrow = TRUE) %>%\n  list() %>%\n  st_polygon() %>% \n    st_sfc(crs = crs(rast_obj))\n}\n\n\n\nImport data and use the functions\n\n\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(ragg)\n\n\ndhm <- terra::rast(\"data-git-lfs/DHM25/DHM200.asc\")\ncrs(dhm) <- \"epsg:21781\"\n\n\nswitzerland_21781 <- sf::read_sf(\"data-git-lfs/swissboundaries/swissBOUNDARIES3D_1_3_TLM_LANDESGEBIET.shp\") %>%\n  st_union() %>%\n  st_transform(21781) \n\nmymask <- st_bbox_rast(dhm) %>%\n  st_buffer(5000) %>%\n  st_difference(switzerland_21781)\n\n\n\n\n\nsf_obj <- create_ridges(dhm,n_lines = 35, fac = 1.1,geom_type = \"POLYGON\")\n\nbg_color <- \"#27363B\"\nfg_color <- \"#EB4960\"\nfamily <- \"FreeMono\"\n\n\n\n\nbbox_switz <- st_bbox(switzerland_21781)\nbbox_switz_enlarge <- st_buffer(st_as_sfc(bbox_switz),50000)\nlims <- st_bbox(bbox_switz_enlarge)\nxlims =  lims[c(\"xmin\",\"xmax\")]\nylims = lims[c(\"ymin\",\"ymax\")]\n\nasp <- diff(ylims)/diff(xlims)\n\n\n\n\n\nmyplot <- ggplot(sf_obj) +\n  geom_sf(color = \"NA\", fill = fg_color)  + \n  geom_sf(data = mymask, color = \"NA\", fill = bg_color) +\n  # geom_sf(data = bbox_switz_enlarge, fill = \"NA\") +\n  ggtext::geom_richtext(aes(x = median(xlims), y = quantile(ylims,0.95), label = \"Topography of Switzerland\"), family = family, fill = NA, label.color = NA, hjust = 0.5, size = 6, color = fg_color)+\n  ggtext::geom_richtext(aes(x = median(xlims), y = ylims[\"ymin\"], label = \"Data from ©swisstopo<br>visualized by Nils Ratnaweera\"), family = family, fill = NA, label.color = NA, hjust = 0.5, size = 3.5, color = fg_color)+\n  theme_void() +\n  theme(plot.background = element_rect(fill = bg_color,color = NA)) +\n  coord_sf(datum = 21781,xlim =  xlims, ylim = ylims);\n\nmyplot\n\n\n\nggsave(\"minimalistic_topography.png\", myplot,dpi = 600, device = agg_png, width = 15, units = \"cm\")\n\n\n\n\nOriginal (Esp): “Tan linda que duele.”↩︎\n",
    "preview": "posts/2021-08-13-minimalistic-topography/minimalistic_topography.png",
    "last_modified": "2021-10-22T06:48:15+02:00",
    "input_file": {},
    "preview_width": 3543,
    "preview_height": 2787
  }
]
